{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_union, make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_scorer(pipeline):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_test)\n",
    "    print('train score:', pipeline.score(X_train, y_train))\n",
    "    print('accuracy score:', accuracy_score(y_test, preds))\n",
    "    #print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "      <th>ep_title_formatted</th>\n",
       "      <th>airdate</th>\n",
       "      <th>ep_title_y</th>\n",
       "      <th>number</th>\n",
       "      <th>rating</th>\n",
       "      <th>season</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>Resistance is futile.</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>You will disarm your weapons and escort us to ...</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>If you attempt to intervene, we will destroy you.</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>It is malevolent.</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>Destroy it now.</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                               text  \\\n",
       "0   LOCUTUS                              Resistance is futile.   \n",
       "1   LOCUTUS  You will disarm your weapons and escort us to ...   \n",
       "2   LOCUTUS  If you attempt to intervene, we will destroy you.   \n",
       "3   LOCUTUS                                  It is malevolent.   \n",
       "4   LOCUTUS                                    Destroy it now.   \n",
       "\n",
       "  ep_title_formatted      airdate ep_title_y  number  rating  season  index  \n",
       "0           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  \n",
       "1           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  \n",
       "2           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  \n",
       "3           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  \n",
       "4           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SISKO', 'KIRA', 'BASHIR', 'QUARK', 'O'BRIEN', 'ODO', 'DAX', 'WORF',\n",
       "       'GARAK', 'DUKAT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_chars = df['character'].value_counts()[:10].index\n",
    "common_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[sent for sent in df.loc[df['character'] == 'BASHIR']['text'] if len(word_tokenize(sent)) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each in df.loc[df['character'] == 'BASHIR']['text']:\n",
    "#     if len(word_tokenize(each)) > 5:\n",
    "#         print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_chars_df = df.loc[df['character'].isin(common_chars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_array = [len(word_tokenize(line)) > 5 for line in common_chars_df['text']]\n",
    "longer_than_5_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 8 for line in common_chars_df['text']]\n",
    "longer_than_8_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 10 for line in common_chars_df['text']]\n",
    "longer_than_10_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 15 for line in common_chars_df['text']]\n",
    "longer_than_15_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 20 for line in common_chars_df['text']]\n",
    "longer_than_20_df = common_chars_df[count_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_dfs = [longer_than_5_df, longer_than_8_df, longer_than_10_df, longer_than_15_df, longer_than_20_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SISKO', \"O'BRIEN\", 'KIRA', 'ODO', 'QUARK', 'BASHIR', 'DAX',\n",
       "       'DUKAT', 'GARAK', 'WORF'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longer_than_8_df['character'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47421\n",
      "28985\n",
      "20351\n",
      "8091\n",
      "3060\n"
     ]
    }
   ],
   "source": [
    "for lists in list_of_dfs:\n",
    "    print(lists.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_accuracy(df):\n",
    "    return df['character'].value_counts().values[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.200860378313\n",
      "0.205037088149\n",
      "0.205051348828\n",
      "0.205413422321\n",
      "0.199673202614\n"
     ]
    }
   ],
   "source": [
    "for lists in list_of_dfs:\n",
    "    print(baseline_accuracy(lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BASHIR',\n",
       " 'DAX',\n",
       " 'DUKAT',\n",
       " 'GARAK',\n",
       " 'KIRA',\n",
       " \"O'BRIEN\",\n",
       " 'ODO',\n",
       " 'QUARK',\n",
       " 'SISKO',\n",
       " 'WORF']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(longer_than_5_df['character'])\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, confirm a very simple example works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8917756221\n",
      "accuracy score: 0.235745614035\n"
     ]
    }
   ],
   "source": [
    "X = longer_than_5_df['text']\n",
    "y = le.transform(longer_than_5_df['character'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "rfc_pipe = make_pipeline(\n",
    "    CountVectorizer(stop_words='english'),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "classification_scorer(rfc_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_pipe.predict(['test', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BASHIR', 'SISKO', 'GARAK', 'SISKO', 'KIRA'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(rfc_pipe.predict(['diagnosis', 'doctor', 'my dear', 'old man', 'the new generation of annoying quote-RTs is people quote-RTing people that are actually making the point that a bad thing is bad but the quote-RTer misinterpreted it but doesn\\'t bother to delete because the quote RT is doing numbers']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['QUARK'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(rfc_pipe.predict(['There is a drag show called Dragula that does not use the Rob Zombie song as its theme.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temp_predictor(a_string):\n",
    "    print(le.inverse_transform(rfc_pipe.predict([a_string])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifiers to test: RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.892478560382\n",
      "accuracy score: 0.239035087719\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(longer_than_5_df['text'], longer_than_5_df['character'])\n",
    "\n",
    "rfc_pipe = make_pipeline(\n",
    "    CountVectorizer(stop_words='english'),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "classification_scorer(rfc_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     BASHIR       0.26      0.24      0.25      1473\n",
      "        DAX       0.19      0.14      0.16      1043\n",
      "      DUKAT       0.18      0.11      0.14       467\n",
      "      GARAK       0.14      0.09      0.11       497\n",
      "       KIRA       0.18      0.21      0.20      1492\n",
      "    O'BRIEN       0.26      0.25      0.25      1318\n",
      "        ODO       0.19      0.18      0.19      1245\n",
      "      QUARK       0.27      0.26      0.27      1397\n",
      "      SISKO       0.29      0.38      0.33      2404\n",
      "       WORF       0.23      0.17      0.20       520\n",
      "\n",
      "avg / total       0.23      0.24      0.23     11856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, rfc_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BASHIR', 'DAX', 'DUKAT', 'GARAK', 'KIRA', \"O'BRIEN\", 'ODO',\n",
       "       'QUARK', 'SISKO', 'WORF'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.89242232532\n",
      "accuracy score: 0.238866396761\n"
     ]
    }
   ],
   "source": [
    "X = longer_than_5_df['text']\n",
    "y = le.transform(longer_than_5_df['character'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "rfc_pipe = make_pipeline(\n",
    "    CountVectorizer(stop_words='english'),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "classification_scorer(rfc_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.893996907072\n",
      "accuracy score: 0.238444669366\n",
      "None\n",
      "train score: 0.965912227436\n",
      "accuracy score: 0.234993790534\n",
      "None\n",
      "train score: 0.983686038131\n",
      "accuracy score: 0.240762578616\n",
      "None\n",
      "train score: 0.987640079103\n",
      "accuracy score: 0.249134948097\n",
      "None\n",
      "train score: 0.988235294118\n",
      "accuracy score: 0.247058823529\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(rfc_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.897371010825\n",
      "accuracy score: 0.243758434548\n",
      "None\n",
      "train score: 0.969730425982\n",
      "accuracy score: 0.252518283428\n",
      "None\n",
      "train score: 0.983489484374\n",
      "accuracy score: 0.254913522013\n",
      "None\n",
      "train score: 0.990606460119\n",
      "accuracy score: 0.247157686604\n",
      "None\n",
      "train score: 0.986492374728\n",
      "accuracy score: 0.23137254902\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tfid_pipe = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english'),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(tfid_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.20241810769\n",
      "accuracy score: 0.196187584345\n",
      "None\n",
      "train score: 0.205400680835\n",
      "accuracy score: 0.203946460604\n",
      "None\n",
      "train score: 0.203433138964\n",
      "accuracy score: 0.209905660377\n",
      "None\n",
      "train score: 0.208635464733\n",
      "accuracy score: 0.19574888779\n",
      "None\n",
      "train score: 0.203050108932\n",
      "accuracy score: 0.18954248366\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "svc_pipe = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english'),\n",
    "    SVC()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(svc_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.200618585688\n",
      "accuracy score: 0.201585695007\n",
      "None\n",
      "train score: 0.20484865213\n",
      "accuracy score: 0.205602318201\n",
      "None\n",
      "train score: 0.207626285789\n",
      "accuracy score: 0.197327044025\n",
      "None\n",
      "train score: 0.205504284773\n",
      "accuracy score: 0.205140879881\n",
      "None\n",
      "train score: 0.199564270153\n",
      "accuracy score: 0.2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "svc_pipe = make_pipeline(\n",
    "    CountVectorizer(stop_words='english'),\n",
    "    SVC()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(svc_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(X_train)\n",
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner)\n",
    "X = cv.fit_transform(X_train)\n",
    "\n",
    "def LDA_batch(X, n):\n",
    "    \n",
    "    feature_names = cv.get_feature_names()\n",
    "    lda = LatentDirichletAllocation(n_topics=n)\n",
    "    lda.fit(X)\n",
    "    results = pd.DataFrame(lda.components_, columns=feature_names)\n",
    "    print('LDA for {} topics:'.format(n))\n",
    "    for topic in range(n):\n",
    "        print('Topic', topic)\n",
    "        word_list = results.T[topic].sort_values(ascending=False).index\n",
    "        print(' '.join(word_list[0:25]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA for 10 topics:\n",
      "Topic 0\n",
      "someth id happen quadrant that bajoran man believ sisko gamma hold go captain prophet your talk first quark back actual noth two want get reason \n",
      "\n",
      "Topic 1\n",
      "went founder report cargo arm design gone easi hit never known bay wall water sourc breen woman homeworld year train energi surpris date festiv armi \n",
      "\n",
      "Topic 2\n",
      "year cant three two four obrien secur see room night hundr everi last ive might ago one sit get home small defiant spent surviv side \n",
      "\n",
      "Topic 3\n",
      "know time day go im first come life one say new make ask would much bajor cardassia dax us tri cardassian help major feder twenti \n",
      "\n",
      "Topic 4\n",
      "go want dont know think would im like thing get station look back tell ship starfleet us peopl take command offic kind feder well around \n",
      "\n",
      "Topic 5\n",
      "ill next would your youll well bring anyon thought wont order found stand inform might tell could even take peopl realli mean differ sure heart \n",
      "\n",
      "Topic 6\n",
      "littl right kill your put that may bajoran moment destroy interest point well made think member give oh ship let ive famili see busi opportun \n",
      "\n",
      "Topic 7\n",
      "one way could klingon find well ive there get take know may us go dont time need abl sinc wouldnt make come youv got cardassian \n",
      "\n",
      "Topic 8\n",
      "one entir wait bar life turn central could full sloan except thirti dna command situat hear back son fail humanoid section dozen remain onto land \n",
      "\n",
      "Topic 9\n",
      "open messag import experi either arriv door accept murder public give call gul death sens servic provid mark prepar sacrific sever discuss propheci across pleasur \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LDA_batch(X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA for 20 topics:\n",
      "Topic 0\n",
      "way time might youv ive there want thing long got tell never know either feel oh last us made person good use first stay work \n",
      "\n",
      "Topic 1\n",
      "dax best week seven gul curzon probe probabl two promenad dukat guess lieuten doctor cannot record access dock sir univers file produc bajoran three opposit \n",
      "\n",
      "Topic 2\n",
      "keep matter leav would wasnt accord chanc us cardassia bajor peac close aliv major two least treati children hard jake get encount trip kira vedek \n",
      "\n",
      "Topic 3\n",
      "cardassian time bajoran day next five bring well hope begin two level prophet togeth minut weve bodi respons twenti high someth point dominion need empir \n",
      "\n",
      "Topic 4\n",
      "station want know go that see make one right let ill your starfleet war back feder tri day put happen find everi peopl secur everyth \n",
      "\n",
      "Topic 5\n",
      "may believ feder mean tell cant could know dominion seem man help order even she come take see work heart protect attack cardassia there second \n",
      "\n",
      "Topic 6\n",
      "one peopl life klingon us know look thing entir he help even face mani tri becom realis certain prove live human whatev fight day go \n",
      "\n",
      "Topic 7\n",
      "come would wormhol ive first wait time eye open obrien caus side decid door intend control back detect sever dozen attempt fact leav fail surround \n",
      "\n",
      "Topic 8\n",
      "year quark place ago ferengi ive ever come one seen test sinc leader later occup see form board like track two bajor signal propheci id \n",
      "\n",
      "Topic 9\n",
      "three ship theyr could hold afraid transport four cell runabout small gave appear surviv spent morn duti explos aboard back troubl almost beam set civilian \n",
      "\n",
      "Topic 10\n",
      "power read full awar murder relay odd academi plant chanc sens commun water rerout innoc run there secondari whoever meal coil report terribl matrix plasma \n",
      "\n",
      "Topic 11\n",
      "mayb kind chang weapon famili ship coupl camp restor analysi sensor comput base labour particl conduct terrorist breen shakaar number clean somebodi control train imperi \n",
      "\n",
      "Topic 12\n",
      "suppos deal data senat rod obtain vreenak torpedo pad grand armi wound forgeri soldier hopeless great panick cost fashion holorecord optolyth opinion photon provid drug \n",
      "\n",
      "Topic 13\n",
      "death public requir torn resid smell mikel contus rib fractur synthesis sanctuari kubuss dust diabol swift unidentifi retir nigel dunlop foil outrag chessarro autopsi wrongdo \n",
      "\n",
      "Topic 14\n",
      "figur trust meet appreci prison favour soon caught behind earn wind strike admit unlik ask thrill seal proceed gestur airlock uncl agonis bid get compens \n",
      "\n",
      "Topic 15\n",
      "go dont get im well think know your command say take want littl would find like look quadrant thing us turn stop isnt starfleet someth \n",
      "\n",
      "Topic 16\n",
      "im sure like realli id must given odo make youd havent truth wish doesnt carri resist tell wonder watch right mention coloni bajoran zone sleep \n",
      "\n",
      "Topic 17\n",
      "could field couldnt enough creat gener case fire devic month abl signatur emerg warp might effect station wed around old justic subspac pattern need within \n",
      "\n",
      "Topic 18\n",
      "two found stand one sinc hundr old someon went scan founder member arm boy thousand night brought tri around could rule friend year log sit \n",
      "\n",
      "Topic 19\n",
      "knew still drink eat accept past battl buy sinc somehow itd lover speed wine cannon juic recal respond deton scene treasuri divin idea cup prune \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LDA_batch(X, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knife thrust directli left right thorac vertebra perfor lower ventricl heart\n",
      "well apart us bodi discov dna present weve identifi ibudan\n",
      "im sure analys fragment detect trace complex organ structur\n",
      "reconstruct dna sequenc might give us idea\n",
      "that look like there genet drift cant quit put finger\n",
      "mean shoe know dax intim think id find hard resist\n",
      "know peopl say remain ds nine eye ear fellow cardassian\n",
      "ran full neurosynapt comparison dax obrien found tempor lobe patient\n",
      "exampl look hold tricord process occur brain connect stimulu word tricord\n",
      "base level viru found patient blood believ absorb ingest\n"
     ]
    }
   ],
   "source": [
    "for each in longer_than_20_df.loc[longer_than_10_df['character'] == 'BASHIR'][:10]['text']:\n",
    "    print(cleaner(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.425165190496\n",
      "accuracy score: 0.176703778677\n",
      "None\n",
      "train score: 0.435274634281\n",
      "accuracy score: 0.168897474817\n",
      "None\n",
      "train score: 0.555460918561\n",
      "accuracy score: 0.197720125786\n",
      "None\n",
      "train score: 0.480388925511\n",
      "accuracy score: 0.232328225408\n",
      "None\n",
      "train score: 0.485403050109\n",
      "accuracy score: 0.250980392157\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "knc_pipe = make_pipeline(\n",
    "    TfidfVectorizer(preprocessor=cleaner),\n",
    "    KNeighborsClassifier()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(knc_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.932855335302\n",
      "accuracy score: 0.232371794872\n",
      "None\n",
      "train score: 0.980817002484\n",
      "accuracy score: 0.250310473299\n",
      "None\n",
      "train score: 0.987158487846\n",
      "accuracy score: 0.244300314465\n",
      "None\n",
      "train score: 0.9912656559\n",
      "accuracy score: 0.24962926347\n",
      "None\n",
      "train score: 0.99128540305\n",
      "accuracy score: 0.213071895425\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cv_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(cv_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.370729649937\n",
      "accuracy score: 0.134024966262\n",
      "None\n",
      "train score: 0.445671174901\n",
      "accuracy score: 0.130260797571\n",
      "None\n",
      "train score: 0.574461115115\n",
      "accuracy score: 0.128341194969\n",
      "None\n",
      "train score: 0.622445616348\n",
      "accuracy score: 0.133465150766\n",
      "None\n",
      "train score: 0.971677559913\n",
      "accuracy score: 0.154248366013\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cv_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    TruncatedSVD(),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(cv_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.937157317588\n",
      "accuracy score: 0.243421052632\n",
      "None\n",
      "train score: 0.984037169933\n",
      "accuracy score: 0.238167517593\n",
      "None\n",
      "train score: 0.990106794208\n",
      "accuracy score: 0.254520440252\n",
      "None\n",
      "train score: 0.990936058009\n",
      "accuracy score: 0.241720217499\n",
      "None\n",
      "train score: 0.990849673203\n",
      "accuracy score: 0.202614379085\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tfid_pipe = make_pipeline(\n",
    "    TfidfVectorizer(preprocessor=cleaner),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(tfid_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "grid = {\n",
    "    'vec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vec__max_df': [0.6, 0.8, 1],\n",
    "    'rfc__n_estimators': [20, 25, 30],\n",
    "    'rfc__min_samples_leaf': [1, 2, 3]   \n",
    "}\n",
    "\n",
    "pipe_gs = GridSearchCV(pipe, param_grid=grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "   ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'vec__ngram_range': [(1, 1), (1, 2), (1, 3)], 'vec__max_df': [0.6, 0.8, 1], 'rfc__n_estimators': [20, 25, 30], 'rfc__min_samples_leaf': [1, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__min_samples_leaf': 2,\n",
       " 'rfc__n_estimators': 25,\n",
       " 'vec__max_df': 0.8,\n",
       " 'vec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23921568627450981"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__min_samples_leaf': 3,\n",
       " 'rfc__n_estimators': 25,\n",
       " 'vec__max_df': 0.4,\n",
       " 'vec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "grid = {\n",
    "    'vec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vec__max_df': [0.4, 0.6, 0.8],\n",
    "    'rfc__n_estimators': [15, 20, 25],\n",
    "    'rfc__min_samples_leaf': [3, 4, 5]   \n",
    "}\n",
    "\n",
    "pipe_gs = GridSearchCV(pipe, param_grid=grid)\n",
    "pipe_gs.fit(X_train, y_train)\n",
    "pipe_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24183006535947713"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__min_samples_leaf': 3,\n",
       " 'rfc__n_estimators': 30,\n",
       " 'vec__max_df': 0.6,\n",
       " 'vec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "grid = {\n",
    "    'vec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vec__max_df': [0.5, 0.6, 0.7],\n",
    "    'rfc__n_estimators': [20, 25, 30],\n",
    "    'rfc__min_samples_leaf': [3, 4, 5]   \n",
    "}\n",
    "\n",
    "pipe_gs = GridSearchCV(pipe, param_grid=grid)\n",
    "pipe_gs.fit(X_train, y_train)\n",
    "pipe_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24749455337690632"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "grid = {\n",
    "    'vec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vec__max_df': [0.5, 0.6, 0.7],\n",
    "    'rfc__n_estimators': [20, 25, 30],\n",
    "    'rfc__min_samples_leaf': [3, 4, 5]   \n",
    "}\n",
    "pipe_gs = GridSearchCV(pipe, param_grid=grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'rfc__min_samples_leaf': 3, 'rfc__n_estimators': 30, 'vec__max_df': 0.5, 'vec__ngram_range': (1, 2)}\n",
      "best score: 0.271418529453\n",
      "best params: {'rfc__min_samples_leaf': 3, 'rfc__n_estimators': 30, 'vec__max_df': 0.5, 'vec__ngram_range': (1, 1)}\n",
      "best score: 0.269252001104\n",
      "best params: {'rfc__min_samples_leaf': 3, 'rfc__n_estimators': 30, 'vec__max_df': 0.7, 'vec__ngram_range': (1, 1)}\n",
      "best score: 0.265347572561\n",
      "best params: {'rfc__min_samples_leaf': 3, 'rfc__n_estimators': 30, 'vec__max_df': 0.6, 'vec__ngram_range': (1, 1)}\n",
      "best score: 0.257251153593\n",
      "best params: {'rfc__min_samples_leaf': 3, 'rfc__n_estimators': 25, 'vec__max_df': 0.6, 'vec__ngram_range': (1, 1)}\n",
      "best score: 0.251416122004\n"
     ]
    }
   ],
   "source": [
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    pipe_gs.fit(X_train, y_train)\n",
    "    print('best params:', pipe_gs.best_params_)\n",
    "    print('best score:', pipe_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function cleaner at 0x7f1ebccad6a8>, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner)\n",
    "cv.fit(df['text'])\n",
    "to_dense = cv.transform(df['text']).todense()\n",
    "to_dense_df = pd.DataFrame(to_dense, columns=cv.get_feature_names())\n",
    "to_dense_df.sum().sort_values(ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['im', 'go', 'dont', 'know', 'one', 'get', 'your', 'want', 'well',\n",
    "       'like', 'think', 'right', 'that', 'time', 'us', 'see', 'would', 'come',\n",
    "       'take', 'look', 'could', 'ill', 'make', 'way', 'let', 'say', 'tell',\n",
    "       'good', 'back', 'ive', 'need', 'thing', 'cant', 'someth',\n",
    "       'never', 'two', 'tri', 'he', 'ye', 'oh', 'sure', 'talk', 'got', 'find', 'didnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_for_cleaner = stopwords.words('english').append(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stop_words_for_cleaner\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words.extend(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = english_stop_words\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function cleaner at 0x7f1e830e7268>, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner)\n",
    "cv.fit(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "your          3112\n",
       "go            2905\n",
       "that          2430\n",
       "command       1347\n",
       "someth        1346\n",
       "ship          1275\n",
       "tri           1271\n",
       "he            1241\n",
       "ye            1214\n",
       "station       1154\n",
       "cardassian    1140\n",
       "peopl         1139\n",
       "work          1076\n",
       "there         1075\n",
       "mean          1060\n",
       "doctor        1049\n",
       "theyr         1033\n",
       "day           1003\n",
       "help           998\n",
       "captain        984\n",
       "much           967\n",
       "sir            957\n",
       "realli         933\n",
       "first          931\n",
       "id             928\n",
       "give           927\n",
       "still          916\n",
       "mayb           912\n",
       "three          905\n",
       "odo            904\n",
       "better         902\n",
       "even           899\n",
       "use            890\n",
       "quark          884\n",
       "major          871\n",
       "littl          862\n",
       "noth           855\n",
       "chief          847\n",
       "klingon        847\n",
       "thank          830\n",
       "thought        823\n",
       "anyth          820\n",
       "long           818\n",
       "believ         811\n",
       "ask            801\n",
       "year           800\n",
       "happen         800\n",
       "feel           795\n",
       "sisko          789\n",
       "must           785\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dense = cv.transform(df['text']).todense()\n",
    "to_dense_df = pd.DataFrame(to_dense, columns=cv.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['your', 'go', 'that', 'command', 'someth', 'ship', 'tri', 'he', 'ye',\n",
       "       'station', 'cardassian', 'peopl', 'work', 'there', 'mean', 'doctor',\n",
       "       'theyr', 'day', 'help', 'captain', 'much', 'sir', 'realli', 'first',\n",
       "       'id', 'give', 'still', 'mayb', 'three', 'odo', 'better', 'even', 'use',\n",
       "       'quark', 'major', 'littl', 'noth', 'chief', 'klingon', 'thank',\n",
       "       'thought', 'anyth', 'long', 'believ', 'ask', 'year', 'happen', 'feel',\n",
       "       'sisko', 'must'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dense_df.sum().sort_values(ascending=False).index[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words.extend(['your', 'go', 'that', 'someth', 'he', 'ye', 'there', 'peopl', 'theyr', 'much', 'realli', 'id', 'give', 'still', 'mayb', 'three', 'better', 'even', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function cleaner at 0x7f1e830e7268>, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner)\n",
    "cv.fit(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_dense = cv.transform(df['text']).todense()\n",
    "to_dense_df = pd.DataFrame(to_dense, columns=cv.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_word_counts = to_dense_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sum_of_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "your          3112\n",
       "go            2905\n",
       "that          2430\n",
       "command       1347\n",
       "someth        1346\n",
       "ship          1275\n",
       "tri           1271\n",
       "he            1241\n",
       "ye            1214\n",
       "station       1154\n",
       "cardassian    1140\n",
       "peopl         1139\n",
       "work          1076\n",
       "there         1075\n",
       "mean          1060\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dense_df.sum().sort_values(ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(cv.transform(df['text']).todense(), \n",
    "             columns=cv.get_feature_names()).sum().sort_values(ascending=False)[:15].plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = longer_than_20_df\n",
    "\n",
    "X = l['text']\n",
    "y = le.transform(l['character'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "pipe_gs.fit(X_train, y_train)\n",
    "print('best params:', pipe_gs.best_params_)\n",
    "print('best score:', pipe_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = longer_than_15_df\n",
    "\n",
    "X = l['text']\n",
    "y = le.transform(l['character'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "pipe_gs.fit(X_train, y_train)\n",
    "print('best params:', pipe_gs.best_params_)\n",
    "print('best score:', pipe_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = longer_than_10_df\n",
    "\n",
    "X = l['text']\n",
    "y = le.transform(l['character'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "pipe_gs.fit(X_train, y_train)\n",
    "print('best params:', pipe_gs.best_params_)\n",
    "print('best score:', pipe_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pipe = make_pipeline(TfidfVectorizer(preprocessor=cleaner),\n",
    "           MLPClassifier()\n",
    ")\n",
    "\n",
    "l = longer_than_20_df\n",
    "\n",
    "classification_scorer(mlp_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(cv_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
