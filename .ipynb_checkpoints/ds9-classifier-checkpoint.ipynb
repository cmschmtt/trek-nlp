{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS9 Character Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_union, make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_scorer(pipeline):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_test)\n",
    "    print('train score:', pipeline.score(X_train, y_train))\n",
    "    print('accuracy score:', accuracy_score(y_test, preds))\n",
    "    #print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "      <th>ep_title_formatted</th>\n",
       "      <th>airdate</th>\n",
       "      <th>ep_title_y</th>\n",
       "      <th>number</th>\n",
       "      <th>rating</th>\n",
       "      <th>season</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>Resistance is futile.</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>You will disarm your weapons and escort us to ...</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>If you attempt to intervene, we will destroy you.</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>It is malevolent.</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOCUTUS</td>\n",
       "      <td>Destroy it now.</td>\n",
       "      <td>emissary</td>\n",
       "      <td>3 Jan. 1993</td>\n",
       "      <td>Emissary</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                               text  \\\n",
       "0   LOCUTUS                              Resistance is futile.   \n",
       "1   LOCUTUS  You will disarm your weapons and escort us to ...   \n",
       "2   LOCUTUS  If you attempt to intervene, we will destroy you.   \n",
       "3   LOCUTUS                                  It is malevolent.   \n",
       "4   LOCUTUS                                    Destroy it now.   \n",
       "\n",
       "  ep_title_formatted      airdate ep_title_y  number  rating  season  index  \n",
       "0           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  \n",
       "1           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  \n",
       "2           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  \n",
       "3           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  \n",
       "4           emissary  3 Jan. 1993   Emissary       1     7.5       1      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SISKO', 'KIRA', 'BASHIR', 'QUARK', 'O'BRIEN', 'ODO', 'DAX', 'WORF',\n",
       "       'GARAK', 'DUKAT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_chars = df['character'].value_counts()[:10].index\n",
    "common_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_chars_df = df.loc[df['character'].isin(common_chars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_array = [len(word_tokenize(line)) > 5 for line in common_chars_df['text']]\n",
    "longer_than_5_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 8 for line in common_chars_df['text']]\n",
    "longer_than_8_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 10 for line in common_chars_df['text']]\n",
    "longer_than_10_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 15 for line in common_chars_df['text']]\n",
    "longer_than_15_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 20 for line in common_chars_df['text']]\n",
    "longer_than_20_df = common_chars_df[count_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_dfs = [longer_than_5_df, longer_than_8_df, longer_than_10_df, longer_than_15_df, longer_than_20_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47421\n",
      "28985\n",
      "20351\n",
      "8091\n",
      "3060\n"
     ]
    }
   ],
   "source": [
    "for lists in list_of_dfs:\n",
    "    print(lists.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_accuracy(df):\n",
    "    return df['character'].value_counts().values[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.200860378313\n",
      "0.205037088149\n",
      "0.205051348828\n",
      "0.205413422321\n",
      "0.199673202614\n"
     ]
    }
   ],
   "source": [
    "for lists in list_of_dfs:\n",
    "    print(baseline_accuracy(lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BASHIR',\n",
       " 'DAX',\n",
       " 'DUKAT',\n",
       " 'GARAK',\n",
       " 'KIRA',\n",
       " \"O'BRIEN\",\n",
       " 'ODO',\n",
       " 'QUARK',\n",
       " 'SISKO',\n",
       " 'WORF']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(longer_than_5_df['character'])\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you      31232\n",
       "the      30496\n",
       "to       27166\n",
       "it       14904\n",
       "of       12247\n",
       "that     12071\n",
       "and       9907\n",
       "we        8881\n",
       "is        7783\n",
       "in        7490\n",
       "have      6972\n",
       "me        6854\n",
       "what      6801\n",
       "be        6342\n",
       "for       6295\n",
       "this      5724\n",
       "re        5538\n",
       "not       5535\n",
       "he        5435\n",
       "on        5387\n",
       "your      5014\n",
       "do        4918\n",
       "but       4745\n",
       "my        4729\n",
       "are       4593\n",
       "they      4592\n",
       "can       4533\n",
       "don       4522\n",
       "was       4513\n",
       "all       4180\n",
       "with      4177\n",
       "ll        3946\n",
       "know      3938\n",
       "if        3809\n",
       "no        3771\n",
       "there     3650\n",
       "ve        3486\n",
       "about     3448\n",
       "one       3219\n",
       "here      3075\n",
       "just      3054\n",
       "so        2993\n",
       "going     2906\n",
       "as        2777\n",
       "get       2662\n",
       "him       2598\n",
       "right     2532\n",
       "like      2530\n",
       "will      2506\n",
       "at        2497\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(df['text'])\n",
    "to_dense = cv.transform(df['text']).todense()\n",
    "to_dense_df = pd.DataFrame(to_dense, columns=cv.get_feature_names())\n",
    "to_dense_df.sum().sort_values(ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list.extend(['im', 'go', 'dont', 'know', 'get', 'one', 'want', 'well', 'your',\n",
    "       'think', 'like', 'us', 'would', 'take', 'that', 'see', 'could',\n",
    "       'right', 'way', 'make', 'ill', 'say', 'ive', 'tell', 'back',\n",
    "       'let', 'come', 'thing', 'cant', 'tri', 'two',\n",
    "       'someth', 'there', 'find', 'talk', 'got', 'didn\\t', 'sure', 'he', 'id', 'work'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list.extend(['go', 'your', 'theyr', 'day', 'much', 'use', 'still', 'mean', 'thought', 'oh', 'anyth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infrequent_words = to_dense_df.sum()[to_dense_df.sum() < 5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_list.extend(infrequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12373"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stop_words_list \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.893434556446\n",
      "accuracy score: 0.237938596491\n"
     ]
    }
   ],
   "source": [
    "# toy example to check for bugs\n",
    "X = longer_than_5_df['text']\n",
    "y = le.transform(longer_than_5_df['character'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "rfc_pipe = make_pipeline(\n",
    "    CountVectorizer(stop_words='english'),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "classification_scorer(rfc_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8684943062\n",
      "accuracy score: 0.23895074224\n"
     ]
    }
   ],
   "source": [
    "X = longer_than_5_df['text']\n",
    "y = le.transform(longer_than_5_df['character'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "rfc_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "classification_scorer(rfc_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SISKO', 'SISKO', 'KIRA', 'SISKO', 'SISKO'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(rfc_pipe.predict(['diagnosis', 'doctor', 'my dear', 'old man', 'testing a longer string']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.24      0.24      1451\n",
      "          1       0.20      0.15      0.18      1008\n",
      "          2       0.16      0.11      0.13       443\n",
      "          3       0.12      0.08      0.09       532\n",
      "          4       0.19      0.20      0.19      1522\n",
      "          5       0.25      0.23      0.24      1359\n",
      "          6       0.19      0.16      0.17      1313\n",
      "          7       0.28      0.27      0.28      1386\n",
      "          8       0.28      0.41      0.33      2337\n",
      "          9       0.22      0.17      0.19       505\n",
      "\n",
      "avg / total       0.23      0.24      0.23     11856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, rfc_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.870743708702\n",
      "accuracy score: 0.237769905533\n",
      "None\n",
      "train score: 0.949949397369\n",
      "accuracy score: 0.244928936111\n",
      "None\n",
      "train score: 0.974972154884\n",
      "accuracy score: 0.242727987421\n",
      "None\n",
      "train score: 0.987640079103\n",
      "accuracy score: 0.253583786456\n",
      "None\n",
      "train score: 0.988235294118\n",
      "accuracy score: 0.245751633987\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(rfc_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.872205820329\n",
      "accuracy score: 0.249240890688\n",
      "None\n",
      "train score: 0.956573741835\n",
      "accuracy score: 0.248240651304\n",
      "None\n",
      "train score: 0.97837908668\n",
      "accuracy score: 0.253144654088\n",
      "None\n",
      "train score: 0.987804878049\n",
      "accuracy score: 0.244686109738\n",
      "None\n",
      "train score: 0.98779956427\n",
      "accuracy score: 0.239215686275\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tfid_pipe = make_pipeline(\n",
    "    TfidfVectorizer(preprocessor=cleaner),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(tfid_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.89748348095\n",
      "accuracy score: 0.248313090418\n",
      "None\n",
      "train score: 0.971892538412\n",
      "accuracy score: 0.25307023596\n",
      "None\n",
      "train score: 0.985258468191\n",
      "accuracy score: 0.240959119497\n",
      "None\n",
      "train score: 0.990771259064\n",
      "accuracy score: 0.237765694513\n",
      "None\n",
      "train score: 0.98779956427\n",
      "accuracy score: 0.239215686275\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tfid_pipe = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english'),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "for l in list_of_dfs:\n",
    "    X = l['text']\n",
    "    y = le.transform(l['character'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    print(classification_scorer(tfid_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.481872116018\n",
      "accuracy score: 0.283242708848\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(longer_than_15_df['text'], longer_than_15_df['character'])\n",
    "\n",
    "lr_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    LogisticRegressionCV()\n",
    ")\n",
    "\n",
    "classification_scorer(lr_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.561140408701\n",
      "accuracy score: 0.29115175482\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(longer_than_15_df['text'], le.transform(longer_than_15_df['character']))\n",
    "\n",
    "lr_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    LogisticRegressionCV()\n",
    ")\n",
    "\n",
    "classification_scorer(lr_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.989452867502\n",
      "accuracy score: 0.252100840336\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(longer_than_15_df['text'], le.transform(longer_than_15_df['character']))\n",
    "\n",
    "rfc_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "classification_scorer(rfc_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'countvectorizer', 'randomforestclassifier', 'countvectorizer__analyzer', 'countvectorizer__binary', 'countvectorizer__decode_error', 'countvectorizer__dtype', 'countvectorizer__encoding', 'countvectorizer__input', 'countvectorizer__lowercase', 'countvectorizer__max_df', 'countvectorizer__max_features', 'countvectorizer__min_df', 'countvectorizer__ngram_range', 'countvectorizer__preprocessor', 'countvectorizer__stop_words', 'countvectorizer__strip_accents', 'countvectorizer__token_pattern', 'countvectorizer__tokenizer', 'countvectorizer__vocabulary', 'randomforestclassifier__bootstrap', 'randomforestclassifier__class_weight', 'randomforestclassifier__criterion', 'randomforestclassifier__max_depth', 'randomforestclassifier__max_features', 'randomforestclassifier__max_leaf_nodes', 'randomforestclassifier__min_impurity_decrease', 'randomforestclassifier__min_impurity_split', 'randomforestclassifier__min_samples_leaf', 'randomforestclassifier__min_samples_split', 'randomforestclassifier__min_weight_fraction_leaf', 'randomforestclassifier__n_estimators', 'randomforestclassifier__n_jobs', 'randomforestclassifier__oob_score', 'randomforestclassifier__random_state', 'randomforestclassifier__verbose', 'randomforestclassifier__warm_start'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'randomforestclassifier__n_estimators': [5, 10, 15],\n",
    "    'randomforestclassifier__max_depth': [None, 5, 10],\n",
    "    'randomforestclassifier__min_samples_split': [2, 5],\n",
    "    'randomforestclassifier__min_samples_leaf': [1, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(rfc_pipe, params_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.8s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   19.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.7s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.7s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.9s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.9s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.9s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  12.1s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  12.1s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  12.1s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.7s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.8s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.8s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.8s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.9s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.9s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.9s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=None, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=5, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=1, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=10, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=2, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=5, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=10, total=  11.6s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n",
      "[CV] randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15 \n",
      "[CV]  randomforestclassifier__max_depth=10, randomforestclassifier__min_samples_leaf=3, randomforestclassifier__min_samples_split=5, randomforestclassifier__n_estimators=15, total=  11.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed: 34.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function cleaner...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'randomforestclassifier__n_estimators': [5, 10, 15], 'randomforestclassifier__max_depth': [None, 5, 10], 'randomforestclassifier__min_samples_split': [2, 5], 'randomforestclassifier__min_samples_leaf': [1, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__max_depth': None,\n",
       " 'randomforestclassifier__min_samples_leaf': 3,\n",
       " 'randomforestclassifier__min_samples_split': 5,\n",
       " 'randomforestclassifier__n_estimators': 15}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27653263019116675"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.532300593276\n",
      "accuracy score: 0.29115175482\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(longer_than_15_df['text'], le.transform(longer_than_15_df['character']))\n",
    "\n",
    "lr_pipe = make_pipeline(\n",
    "    TfidfVectorizer(preprocessor=cleaner),\n",
    "    LogisticRegressionCV()\n",
    ")\n",
    "\n",
    "classification_scorer(lr_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(longer_than_15_df['text'], le.transform(longer_than_15_df['character']))\n",
    "\n",
    "lr_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    LogisticRegressionCV()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'countvectorizer__max_df': [0.8, 1.0],\n",
    "    'logisticregressioncv__fit_intercept': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(lr_pipe, params_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=True \n",
      "[CV]  countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=True, total=  24.0s\n",
      "[CV] countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=True \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=True, total=  24.2s\n",
      "[CV] countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=True \n",
      "[CV]  countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=True, total=  24.1s\n",
      "[CV] countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=False \n",
      "[CV]  countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=False, total=  21.0s\n",
      "[CV] countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=False \n",
      "[CV]  countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=False, total=  20.9s\n",
      "[CV] countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=False \n",
      "[CV]  countvectorizer__max_df=0.8, logisticregressioncv__fit_intercept=False, total=  21.2s\n",
      "[CV] countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=True \n",
      "[CV]  countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=True, total=  24.0s\n",
      "[CV] countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=True \n",
      "[CV]  countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=True, total=  24.2s\n",
      "[CV] countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=True \n",
      "[CV]  countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=True, total=  24.2s\n",
      "[CV] countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=False \n",
      "[CV]  countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=False, total=  21.0s\n",
      "[CV] countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=False \n",
      "[CV]  countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=False, total=  20.9s\n",
      "[CV] countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=False \n",
      "[CV]  countvectorizer__max_df=1.0, logisticregressioncv__fit_intercept=False, total=  21.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function cleaner...2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'countvectorizer__max_df': [0.8, 1.0], 'logisticregressioncv__fit_intercept': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24785761371127224"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " 'countvectorizer__max_df': 1.0,\n",
    " 'countvectorizer__max_features': None,\n",
    " 'countvectorizer__min_df': 1,\n",
    " 'countvectorizer__ngram_range': (1, 1),\n",
    "'logisticregressioncv__fit_intercept': True,\n",
    "    'logisticregressioncv__penalty': 'l2',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(longer_than_15_df['text'], le.transform(longer_than_15_df['character']))\n",
    "\n",
    "# gnb_pipe = make_pipeline(\n",
    "#     CountVectorizer(preprocessor=cleaner),\n",
    "#     GaussianNB()\n",
    "# )\n",
    "\n",
    "# classification_scorer(gnb_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.634146341463\n",
      "accuracy score: 0.302521008403\n"
     ]
    }
   ],
   "source": [
    "mnb_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "classification_scorer(mnb_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "classification_scorer(mnb_pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
