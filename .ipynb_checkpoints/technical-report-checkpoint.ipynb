{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Technical Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caroline Schmitt\n",
    "12/18/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification can be a difficult natural language processing task. Its applications can be broad -- from comparing one's prose style to famous authors[2](https://iwl.me/about/) to identifying speakers over wiretaps[1](https://www.osti.gov/scitech/servlets/purl/11824). For this project I attempted to build a classification model for dialog on the TV show Star Trek: Deep Space Nine. Attempting to classify TV dialog is an especially interesting task because TV shows often have dozens of writers who come and go, some staying for seasons at a time and some writing only one or two episodes, but nonetheless each writer is expected to make long-standing characters sound like themselves; therefore I make the assumption there is true continuity in language patterns for each character throughout all seven seasons of the series.\n",
    "\n",
    "The classes I chose were the ten characters with the most lines through all seven seasons of the series. The baseline accuracy of such a classification model is `.20`. I was able to increase my model's accuracy to `.30` and am confident that with further testing, I can improve it even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data acquisition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used `requests` and `BeautifulSoup` to scrape fan transcripts and converted them to pandas DataFrame, then a `.csv` file for storage. Each sentence of dialog is stored on its own line, tagged with character, season, and the title of the episode that the line was taken from.\n",
    "\n",
    "As I did not transcripe the episodes, I am assuming that the fan transcriptions are accurate to the show. This may be confounded by typos or other data entry-type errors, or by the transcriptionist having misheard something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the scripts had several stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scr = []\n",
    "# 401,576\n",
    "for ep in range(401,576):\n",
    "    url = \"http://www.chakoteya.net/DS9/{}.htm\".format(ep)\n",
    "    try:\n",
    "        scr.append(urllib.request.urlopen(url).read())\n",
    "    except urllib.request.HTTPError as err:\n",
    "        if err.code == 404:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "many_soups = []\n",
    "for ep in scr:\n",
    "    many_soups.append(BeautifulSoup(ep, \"lxml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_tokenizer = nltk.tokenize.sent_tokenize\n",
    "pattern = re.compile(r'(\\b[A-Z]+|([A-Z]+.[A-Z]+))(\\:|\\s\\[.+\\]\\:)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ep in many_soups:\n",
    "    \n",
    "    episode_title = ep.b.string\n",
    "    episode_title = episode_title.replace('\\r\\n', ' ')\n",
    "    \n",
    "    array_of_strings = []\n",
    "    \n",
    "    for string in ep.stripped_strings:\n",
    "        array_of_strings.append(string.replace('\\r\\n', ' '))\n",
    "        \n",
    "    clean_df = []\n",
    "    char_dict = {}\n",
    "\n",
    "    for string in array_of_strings:\n",
    "        found = re.search(pattern, string)\n",
    "        if found is not None:\n",
    "            stripped_string = string.replace(found.group(0), '').strip()\n",
    "            stripped_string_tokenized = sent_tokenizer(stripped_string)\n",
    "\n",
    "            key = found.group(1)\n",
    "\n",
    "            for each in stripped_string_tokenized:\n",
    "                    clean_df.append(each)\n",
    "                    char_dict.setdefault(key, []).append(each)\n",
    "    \n",
    "    for key in char_dict:\n",
    "        temp_df = pd.DataFrame(char_dict[key], columns=['text'])\n",
    "        temp_df['character'] = key\n",
    "        temp_df['ep_title'] = episode_title\n",
    "        df = df.append(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I did with the data was limit it to the characters who spoke the most dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_chars = df['character'].value_counts()[:10].index\n",
    "common_chars_df = df.loc[df['character'].isin(common_chars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also constructed four subsets of the data based on sentence length to test if longer or shorter sentences resulted in more accurate models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_array = [len(word_tokenize(line)) > 5 for line in common_chars_df['text']]\n",
    "longer_than_5_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 8 for line in common_chars_df['text']]\n",
    "longer_than_8_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 10 for line in common_chars_df['text']]\n",
    "longer_than_10_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 15 for line in common_chars_df['text']]\n",
    "longer_than_15_df = common_chars_df[count_array]\n",
    "\n",
    "count_array = [len(word_tokenize(line)) > 20 for line in common_chars_df['text']]\n",
    "longer_than_20_df = common_chars_df[count_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracies for each sub-dataframe were not dramatically different:\n",
    "\n",
    "`0.200860378313\n",
    "0.205037088149\n",
    "0.205051348828\n",
    "0.205413422321\n",
    "0.199673202614`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used `LabelEncoder` to convert my categorical variables into numbers. I also modified a `cleaner` function I had used before to have a substantial stopwords list, including many of the very common words found in the dataset as well as many of the very uncommon words, so as to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stopwords_list\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w.strip() not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cleaner` preprocessor was used in conjunction with `CountVectorizer` and `TfidVectorizer`, both variable transformers that use bag-of-words measures for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a classification task, my outcome variable is 'predicted speaker', and I am optimizing for accuracy. I tested many different models and found that most were only slightly better than baseline, even after gridsearching. The models that stood out in performance were a `LogisticRegression` model and a `MultinomialNB` model.\n",
    "\n",
    "This `MultinomialNB` model had the best validation score of `.30252`. Multinomial naive Bayes is considered a strong model for NLP classification tasks[1](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    MultinomialNB()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` had the second best validation score of `.28324`. Even after gridsearching extensively for optimal hyperparameters, this was the best-performing `LogisticRegression` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=cleaner),\n",
    "    LogisticRegressionCV()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two models indicate approximately .10 and .8 improved scores over baseline, which isn't bad for a 10-class classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next steps for this model will be to evaluate convolutional neural network models; the size of the dataset means this will take some time. I plan to further experiment with stop words and tokenization as well as adding more variables (such as polarity.)\n",
    "\n",
    "Additionally I would like to select the most 'generic' lines in my database to do more EDA on. There are likely some sentences that multiple characters say throughout the course of the series, such as \"Get help\" or other such generic lines, and it may be that identifying and removing such lines improves the predictive accuracy of my model -- so long as such feature selection and engineering doesn't veer into something like p-hacking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
