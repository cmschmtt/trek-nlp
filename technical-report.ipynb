{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Technical Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caroline Schmitt\n",
    "12/18/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification can be a difficult natural language processing task. Its applications can be broad -- from comparing one's prose style to famous authors[2](https://iwl.me/about/) to identifying speakers over wiretaps[1](https://www.osti.gov/scitech/servlets/purl/11824). For this project I attempted to build a classification model for dialog on the TV show Star Trek: Deep Space Nine. Attempting to classify TV dialog is an especially interesting task because TV shows often have dozens of writers who come and go, some staying for seasons at a time and some writing only one or two episodes, but nonetheless each writer is expected to make long-standing characters sound like themselves; therefore I make the assumption there is true continuity in language patterns for each character throughout all seven seasons of the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is a rather tricky natural language processing task. Here I attempt to classify character dialogue from\n",
    " Star Trek: Deep Space 9 using various machine learning models as well as several pre-processing techniques. This problem i\n",
    "s of particular interest because long-running TV series may have dozens of writers throughout their course, but those write\n",
    "rs are tasked with making sure recurring characters still sound like themselves. Were the writers successful enough in doin\n",
    "g this that a model will be able to discern between characters?\n",
    "\n",
    "Data and assumptions: I scraped scripts from a fan transcript website and parsed them using BeautifulSoup. I constructed a\n",
    "DataFrame with each sentence labeled with the character, season, and episode title that the line was taken from. I also scr\n",
    "aped IMDB for episode ratings with an eye for future modeling projects.\n",
    "As I did not transcribe the episodes, I am assuming that the fan transcriptions are accurate to the show. This may be confo\n",
    "unded by typos or other data entry-type errors. \n",
    "\n",
    "To transform the data for modeling, I utilized both a CountVectorizer and a TfidVectorizer. These are two different bag-of-\n",
    "words measures for NLP tasks.\n",
    "\n",
    "As this is a classification task, my outcome variable is 'predicted speaker', and I am optimizing for accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the scripts had several stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scr = []\n",
    "# 401,576\n",
    "for ep in range(401,576):\n",
    "    url = \"http://www.chakoteya.net/DS9/{}.htm\".format(ep)\n",
    "    try:\n",
    "        scr.append(urllib.request.urlopen(url).read())\n",
    "    except urllib.request.HTTPError as err:\n",
    "        if err.code == 404:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "many_soups = []\n",
    "for ep in scr:\n",
    "    many_soups.append(BeautifulSoup(ep, \"lxml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_tokenizer = nltk.tokenize.sent_tokenize\n",
    "pattern = re.compile(r'(\\b[A-Z]+|([A-Z]+.[A-Z]+))(\\:|\\s\\[.+\\]\\:)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ep in many_soups:\n",
    "    \n",
    "    episode_title = ep.b.string\n",
    "    episode_title = episode_title.replace('\\r\\n', ' ')\n",
    "    \n",
    "    array_of_strings = []\n",
    "    \n",
    "    for string in ep.stripped_strings:\n",
    "        array_of_strings.append(string.replace('\\r\\n', ' '))\n",
    "        \n",
    "    clean_df = []\n",
    "    char_dict = {}\n",
    "\n",
    "    for string in array_of_strings:\n",
    "        found = re.search(pattern, string)\n",
    "        if found is not None:\n",
    "            stripped_string = string.replace(found.group(0), '').strip()\n",
    "            stripped_string_tokenized = sent_tokenizer(stripped_string)\n",
    "\n",
    "            key = found.group(1)\n",
    "\n",
    "            for each in stripped_string_tokenized:\n",
    "                    clean_df.append(each)\n",
    "                    char_dict.setdefault(key, []).append(each)\n",
    "    \n",
    "    for key in char_dict:\n",
    "        temp_df = pd.DataFrame(char_dict[key], columns=['text'])\n",
    "        temp_df['character'] = key\n",
    "        temp_df['ep_title'] = episode_title\n",
    "        df = df.append(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scraping the scripts, I converted them to a dataframe that stored the line of dialog, the character who spoke it, and the episode it was in, with an eye for future modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My full EDA can be found `here`:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
